title: NPFL114, Lecture 5
class: title, langtech, cc-by-nc-sa

# Convolutional Neural Networks II

## Milan Straka

### March 30, 2020

---
section: Refresh
# Designing and Training Neural Networks

Designing and training a neural network is not a one-shot action,
but instead an iterative procedure.

~~~
- When choosing hyperparameters, it is important to verify that the model
  does not underfit and does not overfit.

~~~
- Underfitting can be checked by increasing model capacity or training longer.

~~~
- Overfitting can be tested by observing train/dev difference and by trying
  stronger regularization.

~~~
Specifically, this implies that:
- We need to set number of training epochs so that training loss/performance
  no longer increases at the end of training.

~~~
- Generally, we want to use a large batchsize that does not slow us down too
  much (GPUs sometimes allow larger batches without slowing down training).
  However, with increasing batch size we need to increase learning rate, which
  is possible only to some extent. Also, small batch size sometimes work
  as regularization (especially for vanilla SGD algorithm).

---
# Main Takeaways From Previous Lecture

- Convolutions can provide

  - local interactions in spacial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3×3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double number of channels.

~~~
- Final fully connected layers are not needed, global average pooling
  is usually enough.

~~~
- Batch normalization is a great regularization method for CNNs, allowing
  removal of dropout.

~~~
- Small weight decay (i.e., L2 regularization) of usually 1e-4 is still useful
  for regularizing convolutional kernels.

---
# ResNet – 2015 (3.6% error)

![w=95%,h=center](../04/resnet_depth_effect.pdf)

---
# ResNet – 2015 (3.6% error)

![w=90%,h=center](../04/resnet_block.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%](../04/resnet_block_reduced.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%](../04/resnet_architecture.pdf)

---
# ResNet – 2015 (3.6% error)

![w=42%,mw=50%,h=center,f=left](../04/resnet_overall.pdf)

~~~
The residual connections cannot be applied directly when
number of channels increase.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1×1$ convolution is used on the projections to match the
required number of channels.

---
# ResNet – 2015 (3.6% error)

![w=100%,v=middle](../04/resnet_residuals.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
# ResNet – 2015 (3.6% error)

Training details:
- batch normalizations after each convolution and before activation

~~~
- SGD with batch size 256 and momentum of 0.9

~~~
- learning rate starts with 0.1 and is divided by 10 when error plateaus

~~~
- no dropout, weight decay 0.0001

~~~
- during testing, 10-crop evaluation strategy is used, averaging scores across
  multiple scales – the images are resized so that their smaller size is in
  $\{224, 256, 384, 480, 640\}$

---
class: middle
# ResNet – 2015 (3.6% error)

![w=49%](../04/resnet_validation.pdf)
![w=49%](../04/resnet_testing.pdf)

---
section: ResNetModifications
# ResNet Ablations – Shortcuts

The authors of ResNet published an ablation study several months after the
original paper.

~~~
![w=45%](resnet_ablation_shortcuts.pdf)![w=95%,h=right,mw=52%](resnet_ablation_shortcuts_results.pdf)

---
# ResNet Ablations – Activations

![w=65%,h=center](resnet_ablation_activations.pdf)
![w=45%,h=center](resnet_ablation_activations_results.pdf)

---
# ResNet Ablations – Pre-Activation Results

The _pre-activation_ architecture was evaluated also on ImageNet, in a single-crop
regime.

![w=80%,h=center,mh=80%,v=middle](resnet_preactivation_results.pdf)

---
# WideNet

![w=100%,v=middle](widenet_block.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors do not consider bottleneck blocks. Instead, they experiment with
  different _block types_, e.g., $B(1, 3, 1)$ or $B(3, 3)$

![w=50%](widenet_ablation_blocks.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors evaluate various _widening factors_ $k$

![w=50%](widenet_ablation_width.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors measure the effect of _dropping out_ inside the residual block
  (but not the residual connection itself)

![w=50%](widenet_ablation_dropout.pdf)
![w=70%,h=center](widenet_curves.pdf)

---
# WideNet – Results

| Dataset  | Results |
|----------|:-------:|
| CIFAR    | ![w=90%,mw=50%,h=center](widenet_cifar.pdf) |
| ImageNet | ![w=50%](widenet_imagenet.pdf) |

---
# DenseNet

![w=100%](densenet_overview_2.pdf)
![w=45%,h=center](densenet_overview.pdf)

---
# DenseNet – Architecture

The initial convolution generates 64 channels, each $1×1$ convolution in dense
block 256, each $3×3$ convolution in dense block 32, and the transition layer
reduces the number of channels in the initial convolution by half.

![w=80%,h=center](densenet_architecture.pdf)

---
class: middle
# DenseNet – Results

![w=60%](densenet_results.pdf)![w=40%](densenet_comparison.pdf)

---
# PyramidNet

![w=100%,v=middle](pyramidnet_blocks.pdf)

---
# PyramidNet – Growth Rate

![w=70%,h=center](pyramidnet_growth_rate.pdf)

In architectures up until now, number of filters doubled when spacial
resolution was halved.

~~~
Such exponential growth would suggest gradual widening rule
$D_k = \lfloor D_{k-1} ⋅ α^{1/N}\rfloor$.

~~~
However, the authors employ a linear widening rule
$D_k = \lfloor D_{k-1} + α/N\rfloor$, where $D_k$ is number of filters
in the $k$-th out of $N$ convolutional block and $α$ is number of filters
to add in total.

---
# PyramidNet – Residual Connections

No residual connection can be a real identity – the authors propose
to zero-pad missing channels, where the zero-pad channels correspond
to newly computed features.


![w=85%,h=center](pyramidnet_residuals.pdf)

---
class: middle
# PyramidNet – CIFAR Results

![w=70%](pyramidnet_cifar.pdf)![w=30%](pyramidnet_architecture.pdf)

---
# PyramidNet – ImageNet Results

![w=100%,v=middle](pyramidnet_imagenet.pdf)

---
# ResNeXt

![w=80%,h=center](resnext_block.pdf)

---
# ResNeXt

![w=50%,h=center](resnext_architecture.pdf)

---
# ResNeXt

![w=100%,v=middle](resnext_training.pdf)

---
# ResNeXt

![w=70%,mw=49%,h=center](resnext_ablations_same.pdf)
~~~
![w=70%,mw=49%,h=center](resnext_ablations_double.pdf)

~~~
![w=40%,h=center,mh=55%,v=middle](resnext_imagenet.pdf)

---
section: CNNRegularization
# Deep Networks with Stochastic Depth

![w=70%,h=center](stochastic_depth_illustration.pdf)

We drop a whole block (but not the residual connection) with probability $1-p_l$.
During inference, we multiply the block output by $p_l$ to compensate.

~~~
All $p_l$ can be set to a constant, but more effective is to use a simple linear
decay $p_l = 1 - l/L(1-p_L)$ where $p_L$ is the final probability of the last layer,
motivated by the intuition that the initial blocks extract low-level features
utilized by the later layers and should therefore be present.

---
# Deep Networks with Stochastic Depth

![w=100%](stochastic_depth_ablations.pdf)

~~~
According to the ablation experiments, linear decay with $p_L=0.5$ was selected.

---
# Deep Networks with Stochastic Depth

![w=100%,v=middle](stochastic_depth_cifar.pdf)

---
# Cutout

![w=60%,h=center](cutout_examples.pdf)

Drop $16×16$ square in the input image, with randomly chosen center.
The pixels are replaced by a their mean value from the dataset.

---
# Cutout

![w=80%,h=center](cutout_ablations.pdf)
![w=80%,h=center](cutout_results.pdf)

---
# DropBlock

Dropout drops individual values, SpatialDropout drops whole channels, DropBlock
drops rectangular areas independently in every channel.

![w=100%](dropblock_motivation.pdf)

---
# DropBlock

![w=70%,h=center](dropblock_algorithm.pdf)

---
# DropBlock

The authors have chosen _block size=7_ and also employ linear schedule of the
_keep probability_, which starts at 1 and linearly decays until the target value
is reached at the end of training.

![w=100%](dropblock_ablations.pdf)

---
# DropBlock

![w=100%,v=middle](dropblock_imagenet.pdf)

---
section: EfficientNet
# Squeeze and Excitation

![w=95%,mw=60%,h=right,f=right](senet_illustration.pdf)
![w=95%,mw=60%,h=right,f=right](senet_squeeze_excitation.pdf)

The ILSVRC 2017 winner was SENet, _Squeeze and Excitation Network_,
which augments existing architectures by a squeeze and excitation block.

~~~
- **squeeze (global information embedding)** computes the average value of every
  channel

~~~
- **excitation (adaptive recalibration)** computes a weight for every channel
  using a sigmoid activation function and multiplies the corresponding channel
  with it

  To not increase the number of parameters too much (by $C^2$), a an additional
  small hidden layer with $C/16$ neurons is employed.

---
# Mobile Inverted Bottleneck Convolution

When designing convolutional neural networks for mobile phones, the following
_mobile inverted bottleneck_ block was proposed.

![w=58%,f=right](mbconv_illustration.pdf)

~~~
- Regular convolution is replaced by **separable convolution**,
  which consists of

  - a **depthwise separable** convolution (for example $3×3$) acting on each channel
    separately (which reduces time and space complexity of a regular convolution
    by a factor equal to the number of channels)
  - a $1×1$ convolution acting on each position independently (which reduces
    time and space complexity of a regular convolution by a factor of $3⋅3$)
~~~
- The residual connections connect bottlenecks (layers with least channels)
~~~
- There is no non-linear activation on the bottlenecks (it would lead to
  loss of information given small capacity of bottlenecks)

---
# Mobile Inverted Bottleneck Convolution

The mobile inverted bottleneck convolution is denoted for example as _MBConv6 k3x3_,
where the 6 denotes expansion factor after the bottleneck and $3×3$ is the
kernel size of the separable convolution.

Furthermore, mobile inverted bottleneck convolution can be augmented with squeeze and excitation blocks.

![w=70%,mw=32%,h=center](mbconv_sepconv.pdf)![w=70%,mw=32%,h=center](mbconv_mbconv6.pdf)![w=70%,mw=32%,h=center](mbconv_mbconv3se.pdf)

---
# EfficientNet

As far as I know, the currently best architecture (as of Jan 2020) for image
recognition is _EfficientNet_.

~~~
![w=50%,f=right](efficientnet_architecture.pdf)

The EfficientNet architecture was created using a multi-objective neural
architecture search that optimized both accuracy and computation complexity.

The resulting network is denoted as **EfficientNet-B0** baseline network.

~~~
It was trained using RMSProp with decay 0.9 and momentum 0.9, weight
decay 1e-5 and initial learning rate 0.256 decayed by 0.97 every 2.4
epochs. Dropout with dropout rate 0.2 is used on the last layer, stochastic
depth with survival probability 0.8 is employed, and
$\operatorname{swish}(→x) ≝ →x ⋅ σ(→x)$ activation function is utilized.

---
# EfficientNet – Compound Scaling

![w=60%,h=center](efficientnet_model_scaling.pdf)

To effectively scale the network, the authors propose a simultaneous increase of
three qualities:
- **width**, which is the number of channels;
- **depth**, which is the number of layers;
- **resolution**, which is the input image resolution.

~~~
By a grid search on a network with double computation complexity, the best
trade-off of scaling width by 1.1, depth by 1.2 and resolution by 1.15 was
found ($1.1^2 ⋅ 1.2 ⋅ 1.15^2 ≈ 2$).

---
# EfficientNet – Results

![w=80%,h=center](efficientnet_imagenet.pdf)

---
# EfficientNet – Results

![w=50%](../01/efficientnet_flops.pdf)![w=50%](../01/efficientnet_size.pdf)

---
section: TransferLearning
# Transfer Learning

In many situations, we would like to utilize a model trained on a different
dataset – generally, this cross-dataset usage is called _transfer learning_.

~~~
In image processing, models trained on ImageNet are frequently used as general
**feature extraction models**.

~~~
The easiest scenario is to take a ImageNet model, drop the last classification
layer, and use the result of the global average pooling as image features.
The ImageNet model is not modified during training.

~~~
For efficiency, we may precompute the image features **once** and reuse it later
many times.

---
# Transfer Learning – Finetuning

After we have successfully trained a network employing an ImageNet model,
we may improve performance further by _finetuning_ – training the full network
including the ImageNet model, allowing the feature extraction to adapt to the
current dataset.

~~~
- The laters after the ImageNet models **should** be already trained to
  convergence.

~~~
- Usually a smaller learning rate is necessary, because the original model
  probably finished training with a very small learning rate. A good starting
  point is one tenth of the original starting learning rate (therefore,
  0.0001 for Adam).

~~~
- We have to think about batch normalization, data augmentation or other
  regularization techniques.

---
section: TransposedConvolution
# Transposed Convolution

So far, the convolution operation produces either an output of the same size,
or it produced a smaller one if stride was larger than one.

~~~
In order to come up with _upscaling convolution_, we start by considering how
a gradient is backpropagated through a fully connected layer and a regular
convolution.

~~~
In a fully connected layer without activation:
- during the forward pass, input $→x$ is multiplied by the weight matrix $⇉W$ as $→x ⋅ ⇉W$;
~~~
- during the backward pass, the gradient $→g$ is multiplied by the _transposed_ weight
  matrix as $→g ⋅ ⇉W^T$.

---
# Transposed Convolution

Analogously, in a convolutional layer without activation:
- during the forward pass, the cross-correlation operation between input $⇶I$
  and kernel $⇶K$ is performed as
  $$(⇶K \star ⇶I)_{i, j, o} = ∑_{m, n, c} ⇶I_{i⋅S + m, j⋅S + n, c} ⇶K_{m, n, c, o};$$

~~~
- during the backward pass, we obtain $⇶G_{i,j,o} = \frac{∂L}{∂ (⇶K \star ⇶I)_{i, j, o}}$
  and we need to backpropagate it to obtain $\frac{∂L}{∂ ⇶I_{i, j, o}}$. It is
  not difficult to show that
  $$\frac{∂L}{∂ ⇶I_{i, j, o}} = ∑_{\substack{i', m\\i'⋅S + m = i}} ∑_{\substack{j', n\\j'⋅S + n = j}} ∑_c ⇶G_{i', j', c} ⇶K_{m, n, o, c}.$$

~~~
  This operation is called **transposed** or **upscaling** convolution and
  stride greater than one makes the output larger, not smaller.

- during the backward pass, the gradient $→g$ is multiplied by the _transposed_ weight
  matrix as $→g ⋅ ⇉W^T$

---
# Transposed Convolution

Given that the transposed convolution must be implemented for efficient
backpropagation of a regular convolution, it is usually available for
direct usage in neural network frameworks.

~~~
It is frequently used to perform upscaling of an image, as an “inverse”
operation to pooling (or convolution with stride $>1$), which is useful for
example in _image segmentation_:

![w=50%,h=center](u_net_architecture.pdf)

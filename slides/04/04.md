title: NPFL114, Lecture 4
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Convolutional Neural Networks

## Milan Straka

### March 23, 2020

---
section: Convolution
class: middle, center
# Going Deeper

# Going Deeper

---
# Convolutional Networks

Consider data with some structure (temporal data, speech, images, ‚Ä¶).

~~~
Unlike densely connected layers, we might want:
~~~
- local interactions;
~~~
- parameter sharing (equal response everywhere);
~~~
- shift invariance.

![w=20%,h=center](shift_invariance.pdf)

---
# 1D Convolution

![w=45%,h=center](1d_convolution.pdf)

---
# 2D Convolution

![w=53%,h=center](convolution_all.pdf)

---
# 2D Convolution

![w=73%,h=center](convolution.png)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $x * w$ is defined as
$$(w * x)(t) = ‚à´x(t - a)w(a)\d a.$$

~~~
![w=100%](windowing_signal.svg)

~~~
![w=100%,mh=35%,v=middle](windowing_applied.svg)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $x * w$ is defined as
$$(w * x)(t) = ‚à´x(t - a)w(a)\d a.$$

For vectors, we have
$$(‚Üíw * ‚Üíx)_t = ‚àë\nolimits_i x_{t-i} w_i.$$

~~~
Convolution operation can be generalized to two dimensions by
$$(‚áâK * ‚áâI)_{i, j} = ‚àë\nolimits_{m, n} ‚áâI_{i-m, j-n} ‚áâK_{m, n}.$$

~~~
Closely related is _cross-corellation_, where $K$ is flipped:
$$(‚áâK \star ‚áâI)_{i, j} = ‚àë\nolimits_{m, n} ‚áâI_{i+m, j+n} ‚áâK_{m, n}.$$

---
section: CNNs
# Convolution Layer

The $K$ is usually called a **kernel** or a **filter**.

~~~
Note that usually we have a whole vector of values for a single pixel,
the so-called **channels**. These single pixel channel values have no longer any
spacial structure, so the kernel contains a different set of weights for every
input dimension, obtaining

![w=10%,f=right](3d_kernel.pdf)

$$(‚á∂K \star ‚á∂I)_{i, j} = ‚àë_{m, n, c} ‚á∂I_{i + m, j + n, c} ‚á∂K_{m, n, c}.$$

~~~
Furthermore, we usually want to be able to specify the output dimensionality
similarly to for example a fully connected layer ‚Äì the number of **output
channels** for every pixel. Each output channel is then the output of an
independent convolution operation, so we can consider $‚á∂K$ to be
a four-dimensional tensor and the convolution if computed as

$$(‚á∂K \star ‚á∂I)_{i, j, o} = ‚àë_{m, n, c} ‚á∂I_{i + m, j + n, c} ‚á∂K_{m, n, c, o}.$$

---
# Convolution Layer

To arrive at the complete convolution layer, we need to specify:
- the width $W$ and height $H$ of the kernel;
~~~
- the number of output channels $F$;
~~~
- the **stride** denoting that every output pixel is computed for
  every every **stride**-th input pixel (i.e., the output is half
  the size if stride is 2).

~~~
Considering an input image with $C$ channels, the convolution layer is then
parametrized by a kernel $‚á∂K$ of total size $W √ó H √ó C √ó F$ and is computed as
$$(‚á∂K \star ‚á∂I)_{i, j, o} = ‚àë_{m, n, c} ‚á∂I_{i‚ãÖS + m, j‚ãÖS + n, c} ‚á∂K_{m, n, c, o}.$$

~~~
Note that while only local interactions are performed in the image spacial dimensions
(width and height), we combine input channels in a fully connected manner.

---
# Convolution Layer

There are multiple padding schemes, most common are:
- `valid`: Only use valid pixels, which causes the result to be smaller than the input.
- `same`: Pad original image with zero pixels so that the result is exactly
  the size of the input.

~~~
There are two prevalent image formats (called `data_format` in TensorFlow):
- `channels_last`: The dimensions of the 4-dimensional image tensor are batch,
  height, width, and channels.

  The original TensorFlow format, faster on CPU.

~~~
- `channels_first`: The dimensions of the 4-dimensional image tensor are batch,
  channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

~~~
In TensorFlow, data is represented using the `channels_last` approach and the
runtime will automatically convert it to `channels_first` if it is more suitable
for available hardware (especially for a GPU).

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling (minor translation invariance)
- Average pooling

![w=60%,h=center](pooling.pdf)

---
section: AlexNet
# High-level CNN Architecture

We repeatedly use the following block:
1. Convolution operation
2. Non-linear activation (usually ReLU)
3. Pooling

![w=90%,h=center](cnn.jpg)

---
# AlexNet ‚Äì 2012 (16.4% error)

![w=100%](alexnet.pdf)

---
# AlexNet ‚Äì 2012 (16.4% error)

Training details:
- 2 GPUs for 5-6 days

~~~
- SGD with batch size 128, momentum 0.9, weight decay 0.0005

~~~
- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

~~~
- ReLU non-linearities

~~~
- dropout with rate 0.5 on fully-connected layers

~~~
- data augmentation using translations and horizontal reflections (choosing random
  $224 √ó 224$ patches from $256 √ó 256$ images)
~~~
  - during inference, 10 patches are used (four corner patches and a center
    patch, as well as their reflections)

---
# AlexNet ‚Äì ReLU vs tanh

![w=60%,h=center](relu_vs_tanh.pdf)

---
# LeNet ‚Äì 1998

AlexNet built on already existing CNN architectures, mostly on LeNet, which
achieved 0.8% test error on MNIST.

![w=100%,mh=80%,v=middle](lenet.pdf)

---
class: middle
# Similarities in V1 and CNNs

![w=100%](gabor_functions.pdf)
The primary visual cortex recognizes Gabor functions.

---
# Similarities in V1 and CNNs

![w=90%,h=center](first_convolutional_layer.pdf)
Similar functions are recognized in the first layer of a CNN.

---
section: Deep Prior
# CNNs as Regularizers ‚Äì Deep Prior

![w=50%,h=center](deep_prior_superresolution.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=95%,h=center](deep_prior_network.png)
![w=20%,h=center](deep_prior_network_input.pdf)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=82%,h=center](deep_prior_inpainting.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=100%,v=middle](deep_prior_inpainting_diversity.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=95%,h=center](deep_prior_inpainting_architecture.jpg)

[Deep Prior paper website with supplementary material](https://dmitryulyanov.github.io/deep_image_prior)

---
section: VGG
# VGG ‚Äì 2014 (6.8% error)

![w=48%,f=left](vgg_architecture.pdf)

![w=70%,mw=50%,h=center](inception3_conv5.png)
![w=95%,mw=50%,h=center,mh=40%,v=middle](vgg_parameters.pdf)

---
# VGG ‚Äì 2014 (6.8% error)

Training detail similar to AlexNet:
- SGD with batch size ~~128~~ 256, momentum 0.9, weight decay 0.0005

- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

- ReLU non-linearities

- dropout with rate 0.5 on fully-connected layers

- data augmentation using translations and horizontal reflections (choosing random
  $224 √ó 224$ patches from $256 √ó 256$ images)

~~~
  - additionally, a multi-scale training and evaluation was performed. During
    training, each image was resized so that its smaller size was equal
    to $S$, which was sampled uniformly from $[256, 512]$
~~~
  - during test time, the image was rescaled three times so that the smaller
    size was $256, 384, 512$, respectively, and the results on the three images
    were averaged

---
# VGG ‚Äì 2014 (6.8% error)

![w=59%,h=center](vgg_test_scale_single.pdf)

~~~
![w=59%,h=center](vgg_test_scale_multi.pdf)

---
# VGG ‚Äì 2014 (6.8% error)

![w=100%,v=middle](vgg_inception_results.pdf)

---
section: Inception
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

Inception block:

![w=95%,h=center](inception_block.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

Inception block with dimensionality reduction:

![w=88%,h=center](inception_block_reduction.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=93%,h=center](inception_architecture.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=10%,h=center](inception_graph.pdf)
Also note the two auxiliary classifiers (they have weight 0.3).

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

Training details:
- SGD with momentum 0.9

~~~
- fixed learning rate schedule of decreasing the learning rate by 4% each
  8 epochs
~~~
- during test time, the image was rescaled four times so that the smaller
  size was $256, 288, 320, 352$, respectively.

~~~
  For each image, the left, center and right square was considered, and from
  each square six crops of size $224 √ó 224$ were extracted (4 corners, middle
  crop and the whole scaled-down square) together with their horizontal flips,
  arriving at $4 ‚ãÖ 3 ‚ãÖ 6 ‚ãÖ 2 = 144$ crops per image

~~~
- 7 independently trained models were ensembled

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=100%,v=middle](inception_ablations.pdf)

---
section: BatchNorm
# Batch Normalization

_Internal covariate shift_ refers to the change in the distributions
of hidden node activations due to the updates of network parameters
during training.

~~~
Let $‚Üíx = (x_1, \ldots, x_d)$ be $d$-dimensional input. We would like to
normalize each dimension as
$$xÃÇ_i = \frac{x_i - ùîº[x_i]}{\sqrt{\Var[x_i]}}.$$

~~~
Furthermore, it may be advantageous to learn suitable scale $Œ≥_i$ and shift $Œ≤_i$ to
produce normalized value
$$y_i = Œ≥_i xÃÇ_i + Œ≤_i.$$

---
# Batch Normalization

Consider a mini-batch of $m$ examples $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$.

_Batch normalizing transform_ of the mini-batch is the following transformation.

<div class="algorithm">

**Inputs**: Mini-batch $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$, $Œµ ‚àà ‚Ñù$<br>
**Outputs**: Normalized batch $(‚Üíy^{(1)}, \ldots, ‚Üíy^{(m)})$
~~~
- $‚ÜíŒº ‚Üê \frac{1}{m} ‚àë_{i = 1}^m ‚Üíx^{(i)}$
~~~
- $‚ÜíœÉ^2 ‚Üê \frac{1}{m} ‚àë_{i = 1}^m (‚Üíx^{(i)} - Œº)^2$
~~~
- $\hat‚Üíx^{(i)} ‚Üê (‚Üíx^{(i)} - ‚ÜíŒº) / \sqrt{œÉ^2 + Œµ}$
~~~
- $‚Üíy^{(i)} ‚Üê ‚ÜíŒ≥ \hat‚Üíx^{(i)} + ‚ÜíŒ≤$
</div>

~~~
Batch normalization is commonly added just before a nonlinearity. Therefore, we
replace $f(‚áâW‚Üíx + ‚Üíb)$ by $f(\textit{BN}(‚áâW‚Üíx))$.

~~~
During inference, $‚ÜíŒº$ and $‚ÜíœÉ^2$ are fixed. They are either precomputed
after training on the whole training data, or an exponential moving average is
updated during training.

---
# Batch Normalization

When a batch normalization is used on a fully connected layer, each neuron
is normalized individually across the minibatch.

~~~
However, for convolutional networks we would like the normalization to
honour their properties, most notably the shift invariance. We therefore
normalize each channel across not only the minibatch, but also across
all corresponding spacial/temporal locations.

![w=70%,h=center](batch_normalization_variants.pdf)

---
# Inception with BatchNorm (4.8% error)

![w=100%,h=center](inception_batchnorm.pdf)

~~~
When using batch normalization, dropout can be removed, which speeds up training
without increasing overfitting. Similarly, weight decay can be reduced
by a factor of 5.

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=90%,mw=61%,h=center](inception3_conv5.png)
![w=90%,mw=37%,h=center](inception3_conv3.png)

---
class: middle
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=32%](inception3_inception_a.pdf)
![w=32%](inception3_inception_b.pdf)
![w=32%](inception3_inception_c.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=55%,h=center](inception3_architecture.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

Training details:
- RMSProp with momentum of $Œ≤=0.9$ and $Œµ=1.0$

~~~
- batch size of 32 for 100 epochs

~~~
- initial learning rate of 0.045, decayed by 6% every two epochs

~~~
- gradient clipping with threshold 2.0 was used to stabilize the training

~~~
- label smoothing was first used in this paper, with $Œ±=0.1$

~~~
- input image size enlarged to $299 √ó 299$

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=60%,h=center](inception3_ablation.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=50%,h=center](inception3_single_model.pdf)
~~~
![w=60%,h=center](inception3_ensemble.pdf)

---
section: ResNet
# ResNet ‚Äì 2015 (3.6% error)

![w=95%,h=center](resnet_depth_effect.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=90%,h=center](resnet_block.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%](resnet_block_reduced.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%](resnet_architecture.pdf)

---
# resnet ‚Äì 2015 (3.6% error)

![w=42%,mw=50%,h=center,f=left](resnet_overall.pdf)

~~~
The residual connections cannot be applied directly when
number of channels increase.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1√ó1$ convolution is used on the projections to match the
required number of channels.

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%,v=middle](resnet_residuals.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
# ResNet ‚Äì 2015 (3.6% error)

Training details:
- batch normalizations after each convolution and before activation

~~~
- SGD with batch size 256 and momentum of 0.9

~~~
- learning rate starts with 0.1 and is divided by 10 when error plateaus

~~~
- no dropout, weight decay 0.0001

~~~
- during testing, 10-crop evaluation strategy is used, averaging scores across
  multiple scales ‚Äì the images are resized so that their smaller size is in
  $\{224, 256, 384, 480, 640\}$

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![w=49%](resnet_validation.pdf)
![w=49%](resnet_testing.pdf)

---
# Main Takeaways

- Convolutions can provide

  - local interactions in spacial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3√ó3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double number of channels.

~~~
- Final fully connected layers are not needed, global average pooling
  is usually enough.

~~~
- Batch normalization is a great regularization method for CNNs, allowing
  removal of dropout.

~~~
- Small weight decay (i.e., L2 regularization) of usually 1e-4 is still useful
  for regularizing convolutional kernels.
